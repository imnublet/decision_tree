{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-kLLOeM_ezw"
   },
   "source": [
    "# A2 - Tree Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxU69EZCfpGx"
   },
   "source": [
    "### Organizational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr_1EsnsfOap"
   },
   "source": [
    "#### What you will learn\n",
    "- Visualizing decision trees\n",
    "- Interpreting trees\n",
    "- Implementing BestSplit rules\n",
    "- Implementing a decision tree\n",
    "- Configuring your tree model using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVHDfmkifTM2"
   },
   "source": [
    "#### What to submit\n",
    "Read this notebook and do the exercises. Then, do the following:\n",
    "\n",
    "* Copy your exercise solutions into the skeleton code in `A2.py`, and you can test it with `A2_test.py`. When you're satisfied that your code in `A2.py` passes the tests, commit and push it back to **Github**.\n",
    "\n",
    "* Make a PDF report answers to the open questions (as well as your graphics). You can use `A2_STUDENTNUMBER.tex` as a template to make it in Overleaf if you wish. It should be named `A2_s1234567890.pdf`, but using your own student number instead of 1234567890. Hand the PDF report in through **Brightspace**.\n",
    "\n",
    "    Your answers don't need to be long, just be to the point. The report should under no circumstances be more than 1 page of text, taking an extra page to make room for graphics is ok.\n",
    "\n",
    "* **If you do not submit both `A2.py` to Github and the PDF report to Brightspace, you will fail this assignment.**\n",
    "\n",
    "* **This is an individual assignment. Your code and report must be your own work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd_MuMEjfeHj"
   },
   "source": [
    "#### How do we grade?\n",
    "* 7 Open questions, including images. You can earn a bonus point for completely answering question 5.    Remember, all figures and tables should have a caption.\n",
    "\n",
    "* We test the functioning of your code with 12 unit tests, one of which (for minority_class) already passes, so 11 tests that are about your work.\n",
    "\n",
    "* 50% of the grade is based on the questions and 50% on the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UB5T4QRdheM"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2MLcamGfzpa"
   },
   "source": [
    "If you need to install graphviz, the following code will work for Colab. You can find more extensive installation instructions for graphviz on different systems [here](https://pypi.org/project/graphviz/).\n",
    "\n",
    "Note that you're installing two things:\n",
    "* A backend, that renders the graphics\n",
    "* A Python frontend that allows you to use it in the middle of your program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Taaw33bKtUEu",
    "outputId": "6ecb7cbb-ce11-4e31-f299-2d87682f12fe"
   },
   "outputs": [],
   "source": [
    "# Installing dependencies (on Colab for example)\n",
    "!apt install -y graphviz  # installingthe graphviz backend on a linux system\n",
    "!pip install graphviz  # installing the python frontend for graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PxC3YLRgFY6"
   },
   "source": [
    "Import (and if necessary, first install) the following packages to get started. You don't need to import other packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFEgy-PttY8g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIpXjM4iCzFw"
   },
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFmTlusjheye"
   },
   "source": [
    "One of the key selling points of decision trees is that they're much easier to interpret than many other models. While for example a neural network is essentially a \"black box\", you can look at a decision tree and trace the logic. \n",
    "\n",
    "First we need a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1q3115XwFPi"
   },
   "outputs": [],
   "source": [
    "# Dataset adapted from https://archive.ics.uci.edu/ml/datasets/Zoo\n",
    "url = 'https://raw.githubusercontent.com/MLCourse-LU/Datasets/main/zoo.csv'\n",
    "df = pd.read_csv(filepath_or_buffer=url, header=0, index_col=0)\n",
    "\n",
    "\n",
    "X = df.iloc[:, 1:-1]  # column 0 has the animal names, the last column has the labels\n",
    "y = df.iloc[:, -1]  # use the last column as labels\n",
    "\n",
    "fn = list(X.columns)  # get the column names of the features\n",
    "cn = y.unique()  # get the different animal labels from the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfHD9GOjW9Rv"
   },
   "source": [
    "Let's get a quick impression of what's in here. **You may need to scroll to the right** to see the \"type\" column which we use as class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FtpZVfeoWjqE",
    "outputId": "ce9342a1-d65b-4fb7-f3c7-af6b6cb2dcba"
   },
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14cWZruqwIpv"
   },
   "source": [
    "We have some data, nice. Now let's build a model. Just for demonstration purposes, we're using the whole dataset, but normally you'd be splitting between training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtJznvdPFcso",
    "outputId": "331b7eeb-f66c-4a24-88df-d4692d1d594f"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)  # train on the WHOLE dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKgmnZ02wV4u"
   },
   "source": [
    "Now we can visualize it in a couple of ways. First as text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KCIVoDl0hkp"
   },
   "source": [
    "### Text visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFK1gleCwZF2",
    "outputId": "7dda6c53-4efa-4c51-893a-74f9dc59b073"
   },
   "outputs": [],
   "source": [
    "# Show the model as text\n",
    "print(export_text(model, feature_names=fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3NLHtuSY6BP"
   },
   "source": [
    "This makes sense - we can see at the bottom for example that if `milk > 0.5` holds, which means the animal produces milk, then it's a mammal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOITRYTlwbE7"
   },
   "source": [
    "### Graphical visualization\n",
    "\n",
    "Then we use graphviz. Graphviz is a graphics package designed for handling graphs - like figuring out where to put notes so it all fits on the paper. This goes in a couple of steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9dYJjBgwRTe",
    "outputId": "0a569b5c-618f-42b9-f6a4-914798c3c516"
   },
   "outputs": [],
   "source": [
    "# Use the sklearn.tree.export_graphviz method to export the tree in \"dot\" format\n",
    "dot = export_graphviz(\n",
    "    decision_tree=model,  # the tree to visualize\n",
    "    out_file=None,  # if None, return it as a string with \"dot\" information\n",
    "    feature_names=fn,  # list of feature names\n",
    "    class_names=sorted(cn),  # sorted! list of class names\n",
    "    filled=True  # pretty colors\n",
    ")\n",
    "\n",
    "print(dot)  # \"dot\" is a string containing a structural description of our future graphic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUfu-Q8lw-Zd"
   },
   "source": [
    "That's not really intended for humans to read, but for transferring it to the graphviz program to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KsceIA3Dw-5p",
    "outputId": "1137b75d-b39d-498a-8d1f-4ecbeaf43a5e"
   },
   "outputs": [],
   "source": [
    "# Load the dot information into a graphviz object\n",
    "graph = graphviz.Source(dot)\n",
    "\n",
    "# Print a copy to disk\n",
    "graph.render('graphviz-tree', format='pdf', cleanup=True)  # save it as a file and clean up helper files\n",
    "\n",
    "# Show the result if using a Notebook\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4rEi9pGc_c9"
   },
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNShI3LtC2C-"
   },
   "source": [
    "Let's analyze this graph. At the top node, we see the following:\n",
    "\n",
    "```\n",
    "milk <= 0.5 \n",
    "gini = 0.759 \n",
    "samples = 101 \n",
    "value = [4, 20, 8, 13, 10, 41, 5] \n",
    "class = Mammal\n",
    "```\n",
    "\n",
    "* The first line, `milk <= 0.5` means that if the value of an instance's \"milk\" feature is 0.5 or lower, it counts as True and goes to the left of the tree, and if False, goes to the right of the tree.\n",
    "\n",
    "* `gini` refers to the Gini index score of this node. This is a score we're trying to minimize. The current score isn't very good.\n",
    "\n",
    "* `Samples = 101` means we have 101 animals in our training data.\n",
    "\n",
    "* `value = [4, 20, 8, 13, 10, 41, 5]` shows us the class distribution of these samples. These are sorted alphabetically by class name, so that is `['Amphibian', 'Bird', 'Bug', 'Fish', 'Invertebrate', 'Mammal', 'Reptile']`. Looks like this zoo has a lot of mammals.\n",
    "\n",
    "* `class` indicates the most likely class for a random sample from this node to belong to. If we grabbed a random animal from the zoo, it's most likely a mammal, since they make up 40% of the animals.\n",
    "\n",
    "Then let's go to the right of the top node. These are the animals for which `milk <= 0.5` was False, in other words, the animal produces milk. We see the following:\n",
    "\n",
    "```\n",
    "gini = 0.0 \n",
    "samples = 41 \n",
    "value = [0, 0, 0, 0, 0, 41, 0] \n",
    "class = Mammal\n",
    "```\n",
    "\n",
    "* We have only 4 lines here, because this node isn't being split further. Why not? Well...\n",
    "* The Gini index is 0. This means there's absolutely no confusion about what class instances in this node belong to.\n",
    "* 41 samples, and only one of the classes in the `value` list has all of those samples. Everything in this node belongs to the same type of animal.\n",
    "* This node predicts Mammals. Which makes sense: mammals are by definition the only kind of animals that produce milk. They're also a significant part of the zoo's population so making this split is a powerful move for the algorithm.\n",
    "\n",
    "We repeat a similar process on the left subtree: only birds have feathers so they're split off. Only fish have scales so they're split off.\n",
    "\n",
    "Then we come to `backbone` and at this point, the tree splits into two true subtrees. Here we can also see that the subtrees don't have to pick the same splitting criteria. Apparently for creatures without backbone (and that don't produce milk, don't have feathers, don't have fins), it's more relevant to ask whether they're airborne, while for creatures with a backbone (and that don't produce milk, don't have feathers, don't have fins) it matters more whether they're aquatic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ-l2s1V2ysH"
   },
   "source": [
    "### Open questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPm6cE3TjwmJ"
   },
   "source": [
    "#### Question 1: Why does this model place \"feathers\" earlier in the tree than \"airborne\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz1UKUTyCV_K"
   },
   "source": [
    "#### Question 2: How could you find the instances in the dataset that end up in a particular leaf, but are misclassified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsp94twYCkmK"
   },
   "source": [
    "## Splitting Rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SJw8T5LhjvF"
   },
   "source": [
    "In a decision tree, we need rules that tell us when to split, and which split is best. For what we use impurity measures, that tell us how badly we're currently doing. If after a split we'd be doing less badly, then that's a good split to apply.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HakdSIs5aEaJ"
   },
   "source": [
    "### Minority class\n",
    "Using minority class as an impurity measure assumes that you classify each node with the majority class of instances in that node. So if the contents of a leaf node are `A, A, A, B, B` you would predict any test examples that end up in that leaf node to have class `A`, but you'd only be right $3/5$th of the time. So this node would have a quality of $1 - 3/5 = 0.4$.\n",
    "\n",
    "If you could split this node so that one node had `A, B, B` and the other `A, A`, then the first one would have a base quality of $2/3$ because we're predicting that node's majority class `B`, and the other a base quality of $2/2$. Adding weights for their size, you'd get $1 - (0.6 \\times \\frac{2}{3} + 0.4 \\times \\frac{2}{2}) = 1 -(0.4 + 0.4) = 0.2$, so this is certainly a split we'd be willing to make.\n",
    "\n",
    "Let's describe this a bit more formally.\n",
    "\n",
    "Let $M$ be the ordered set of classes and $||M||$ the number of classes. $M$ is ordered by frequency. Then $M_i$ is the $i$-th class by frequency, and $M_1$ is the most frequent class. To cover the special case where there is a tie for most frequent class (like in a 50/50 split dataset), we just randomly pick one of them to be indexed as $j=1$.\n",
    "\n",
    "Let $I$ be the set of all  instances in the dataset and $I_j$ the set of instances of class $j$. This allows us to define the probability that a randomly sampled instance belongs to $j$ as:\n",
    "\n",
    "$$p_i = \\frac{||I_j||}{||I||}$$\n",
    "\n",
    "We can now define the minority class impurity measure as:\n",
    "\n",
    "$$minority\\_class = \\sum_{j = 2}^{||M||} p_j$$\n",
    "\n",
    "This is of course equal to $1 - p_1$, the probability of *not* sampling the ranked-first class. Note also that the most frequent class doesn't necessarily cover a majority of the dataset, so the name of this impurity measure is a little bit deceptive when we're not dealing with binary classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gERp3SmDaEta"
   },
   "outputs": [],
   "source": [
    "def minority_class(labels):\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    frequencies = labels.value_counts().values  # array, sorted in descending order\n",
    "    probabilities = [f / len(labels) for f in frequencies[1:]]   # everything except the first class\n",
    "    impurity = sum(probabilities)\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Up8TaaoSZcCq"
   },
   "source": [
    "### Gini index\n",
    "\n",
    "Gini index, also called Gini impurity, should not be confused with Gini coefficient. Both are named after the statistician Corrado Gini.\n",
    "\n",
    "The central idea is as follows. If a node is \"pure\", that is, all instances in it belong to the same class, then predicting the class of an instance is trivial. A pure node has a Gini index of 0. If a node isn't pure yet - meaning it has instances of more than one class in it, then we can calculate just how hard it is to classify instances in that node.\n",
    "\n",
    "Suppose we know the distribution of class labels in a node. For example, we could have a node with `A, A, A, A, A, A, B, B, B, B` in it. Then if we grabbed a random instance, we'd have 60% chance of `A` and 40% chance of `B`. Now suppose we get a new instance and we have to predict its label, and we use those probabilities that we know. So there's a 60% chance that we predict `A`, and 40% chance that we predict `B`. Now, how often are we wrong? That's what Gini impurity measures.\n",
    "\n",
    "The odds of us being wrong are those of predicting `A` but getting `B` ($0.6 \\times 0.4 = 0.24$) and of predicting `B` but getting `A` ($0.4 \\times 0.6 = 0.24$), so a total of $0.24 + 0.24 = 0.48$. This is the same as 1 minus the chance of us getting it right, so $1 - (0.6^2 + 0.4^2) = 0.48$.\n",
    "\n",
    "Now imagine we have a proposed split of this node, that would result in child nodes `A, B, B, B, B` and `A, A, A, A, A`. The odds of getting it wrong in the first node are $1 - (0.2^2 + 0.8^2) = 0.32$. For the other node, the odds of getting it wrong are 0, because there's only one class to choose from. Their weights are each $\\frac{5}{10} = 0.5$ because each node is half the size of the original. So we multiply these impurities with their weights based on instances and get $(0.5 \\times 0.32) + (0.5 \\times 0) = 0.16. This is better than our original impurity of 0.48, so we'd be willing to make this split.\n",
    "\n",
    "Let's formalize Gini Index in a formula:\n",
    "\n",
    "$$Gini = 1 - \\sum_{j=1}^{||M||} p_j^2$$\n",
    "\n",
    "Here $p_j$ is the probability of a given instance being in class $j$, or that we predict an instance to be in class $j$ based on our knowledge of the class distribution in the training set. Then $p_j^2$ is then the probability that an instance is in class $j$ **and** that we predict it as $j$. So then $\\sum_{j=1}^{||M||} p_j^2$ is the sum of all possible ways we could correctly predict an instance's class, and the final formula subtracts that from 1 to get the chance of us being wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr7ZHwDSL_JD"
   },
   "source": [
    "#### Exercise 1: implement Gini impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKDqwqHdMFnv"
   },
   "source": [
    "Using the formila given above, and using the minority class impurity measure as an example, implement a function to calculate the Gini impurity of a set of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eW_M9l0MCrv"
   },
   "outputs": [],
   "source": [
    "def gini(labels):\n",
    "    raise NotImplementedError('Your code here')\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhxclUx_MPwc"
   },
   "source": [
    "And test it with this unit test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNwn5fvEMRq4"
   },
   "outputs": [],
   "source": [
    "def test_gini():\n",
    "    data = pd.Series(['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'])\n",
    "    assert gini(data) == 0.48  # simple two-label\n",
    "    data = pd.Series(['a', 'a', 'a', 'a', 'a', 'c', 'b', 'b', 'b', 'b'])\n",
    "    assert gini(data) == 0.58  # three labels\n",
    "    data = pd.Series(['a', 'a', 'a', 'c', 'c', 'c', 'b', 'b', 'b', 'b'])\n",
    "    assert gini(data) == 0.66  # no absolute majority\n",
    "    data = pd.Series([1, 1, -1, -1])\n",
    "    assert gini(data) == 0.5  # even split, numeric labels\n",
    "    data = pd.Series([1, -1, -1, -1])\n",
    "    assert gini(data) == 0.375  # first instance != majority\n",
    "    data = pd.Series([1, 'a', -1, -1])\n",
    "    assert gini(data) == 0.625  # mixed data types\n",
    "    \n",
    "\n",
    "test_gini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpVou5LnZyYf"
   },
   "source": [
    "### Entropy\n",
    "\n",
    "[Shannon Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) was developed by Claude Shannon in the context of signal processing in lossy communication channels, such as trying to send digital signals over analog radio. The key idea is that you're measuring how much more information you would need to say with certainty which class a subject belongs in. In practice, entropy doesn't behave very differently from Gini index, and they simply came to machine learning out of different scientific fields. Entropy is a bit more computationally expensive to compute because of the logarithm, which is why Gini impurity tends to be the default measure.\n",
    "\n",
    "The formula for entropy is as follows:\n",
    "\n",
    "$$Entropy = - \\sum_j^{||M||} p_j \\cdot log_2(p_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRy4lBeRMe0M"
   },
   "source": [
    "#### Exercise 2: implement entropy impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWCWBFREMhd0"
   },
   "source": [
    "Use the formula given above, and minority class and Gini impurity as examples, and implement entropy as an impurity measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svrQXF3JMoZf"
   },
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    raise NotImplementedError('Your code here')\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaXentEWMrgm"
   },
   "source": [
    "Test your work with this unit test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joUIt2YkMtlq"
   },
   "outputs": [],
   "source": [
    "def test_entropy():\n",
    "    data = pd.Series(['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'])\n",
    "    assert 0.97 < entropy(data) < 0.98  # simple two-label\n",
    "    data = pd.Series(['a', 'a', 'a', 'a', 'a', 'c', 'b', 'b', 'b', 'b'])\n",
    "    assert 1.36 < entropy(data) < 1.37  # three labels\n",
    "    data = pd.Series(['a', 'a', 'a', 'c', 'c', 'c', 'b', 'b', 'b', 'b'])\n",
    "    assert 1.57 < entropy(data) < 1.58  # no absolute majority\n",
    "    data = pd.Series([1, 1, -1, -1])\n",
    "    assert entropy(data) == 1.0  # even split, numeric labels\n",
    "    data = pd.Series([1, -1, -1, -1])\n",
    "    assert 0.81 < entropy(data) < 0.82  # first instance != majority\n",
    "    data = pd.Series([1, 'a', -1, -1])\n",
    "    assert entropy(data) == 1.5  # mixed data types\n",
    "    \n",
    "\n",
    "test_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBgP8oY4dObp"
   },
   "source": [
    "## Building your own tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ldHNUmNCVX8"
   },
   "source": [
    "\n",
    "The main idea of decision trees is simple. It's like a flowchart, telling you to go left if some condition holds, right if otherwise. The difference with a flowchart is that a flowchart is usually designed by an expert, using expert knowledge, while a decision tree is learned from data.\n",
    "\n",
    "You construct a decision tree by repeatedly doing the following:\n",
    "1. For the current node under consideration, determine its impurity.\n",
    "2. Consider the ways (features) in which you could split the node.\n",
    "3. For each possible split, do the following:\n",
    "    * Determine the weight of the child nodes, as a proportion of the size of the parent. So if the parent has 10 instances and a child node would get 6 of those, it has a weight of 6 / 10 = 0.6.\n",
    "    * Determine the impurity for each of the child nodes.\n",
    "    * Sum up the weighted impurity of all the child nodes.\n",
    "4. For all the possible splits you've considered, now look at only the best one. \n",
    "    * If it's also better than the current node, carry out the split, and apply this tree-building algorithm to each of the resulting child nodes.\n",
    "    * Otherwise this is a leaf node.\n",
    "\n",
    "\n",
    "\n",
    "There are a couple of things to notice here:\n",
    "* This algorithm is recursive, because each child node can become the root node of a new subtree. \n",
    "\n",
    "* It's also greedy, because in step 4 we took the best-looking split at that point. \n",
    "\n",
    "* The tree-building algorithm works the same regardless of which impurity function we used.\n",
    "\n",
    "* Nodes with few instances in them have a low weight, and therefore low impact on the weighed impurity of the children compared to the parent node. \n",
    "\n",
    "* In the special case where a split pushes all the instances into only one child node, that single node would gain a weight of 1.0 and the other children would gain a weight of 0.0. \n",
    "\n",
    "    Since the one child with all the instances has the same contents as the parent, computing an impurity measure for it will yield the same result as for the parent, and multiplied by a weight of 1.0, the weighed impurity will be exactly the same as the parent. Since we're looking for an improvement, we would reject this split.\n",
    "\n",
    "    One case where this might happen is if we're considering splitting on a feature that we've already split on. For example, if we've already split animals on \"milk\" and are moving down the \"no\" subtree, splitting on milk again will yield no improvement. This is convenient because it prevents endless growth of the tree. \n",
    "    \n",
    "    Note though that if we were implementing a splitting function for numeric data, this wouldn't hold; just because we've already split on `a <= 0.8` wouldn't mean we couldn't split on `a <= 0.5` for example.\n",
    "\n",
    "\n",
    "\n",
    "In the steps below, we're going to design a decision tree from scratch. We do apply a few simplifications:\n",
    "* The code doesn't have to be perfectly efficient.\n",
    "* We are only going to implement code for splitting on Boolean features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmFc6DhIdbSd"
   },
   "source": [
    "## Assembling the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuKgsW1narZI"
   },
   "source": [
    "\n",
    "Now that we have functions to evaluate splits with, we're ready to implement a decision tree. Because this is a tree data structure, we're going to make heavy use of [recursion](https://www.geeksforgeeks.org/recursion/). Intuitively, a [tree](https://en.wikipedia.org/wiki/Tree_(data_structure)) consists of a root node and a left subtree and a right subtree. We can implement this by designing a class that implements a single node in the tree, with links to child nodes which are again objects of the same class.\n",
    "\n",
    "We'll follow the familiar pattern of model design that we used in Lab1 and A1, with an `__init__`, `fit` and `predict` method, but also a few new ones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjsKqPdr7gdD"
   },
   "outputs": [],
   "source": [
    "class DTree:\n",
    "    def __init__(self, metric):\n",
    "        \"\"\"Set up a new tree.\n",
    "        \n",
    "        We use the `metric` parameter to supply an impurity measure such as Gini or Entropy.\n",
    "        The other class variables should be set by the \"fit\" method.\n",
    "        \"\"\"\n",
    "        self._metric = metric  # what are we measuring impurity with? (Gini, Entropy, Minority Class...)\n",
    "        self._samples = None  # how many training samples reached this node?\n",
    "        self._distribution = []  # what was the class distribution in this node?\n",
    "        self._label = None  # What was the majority class of training samples that reached this node?\n",
    "        self._impurity = None  # what was the impurity at this node?\n",
    "        self._split = False  # if False, then this is a leaf. If you branch from this node, use this to store the name of the feature you're splitting on.\n",
    "        self._yes = None  # Holds the \"yes\" DTree object; None if this is still a leaf node\n",
    "        self._no = None # Holds the \"no\" DTree object; None if this is still a leaf node\n",
    "        \n",
    "\n",
    "    def _best_split(self, features, labels):\n",
    "        \"\"\" Determine the best feature to split on.\n",
    "\n",
    "        :param features: a pd.DataFrame with named training feature columns\n",
    "        :param labels: a pd.Series or pd.DataFrame with training labels\n",
    "        :return: `best_so_far` is a string with the name of the best feature,\n",
    "        and `best_so_far_impurity` is the impurity on that feature\n",
    "\n",
    "        For each candidate feature the weighted impurity of the \"yes\" and \"no\"\n",
    "        instances for that feature are computed using self._metric.\n",
    "\n",
    "        We select the feature with the lowest weighted impurity.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Your code here')\n",
    "        return best_so_far, best_so_far_impurity\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        \"\"\" Generate a decision tree by recursively fitting & splitting them\n",
    "\n",
    "        :param features: a pd.DataFrame with named training feature columns\n",
    "        :param labels: a pd.Series or pd.DataFrame with training labels\n",
    "        :return: Nothing.\n",
    "\n",
    "        First this node is fitted as if it was a leaf node: the training majority label, number of samples,\n",
    "        class distribution and impurity.\n",
    "\n",
    "        Then we evaluate which feature might give the best split.\n",
    "\n",
    "        If there is a best split that gives a lower weighed impurity of the child nodes than the impurity in this node,\n",
    "        initialize the self._yes and self._no variables as new DTrees with the same metric.\n",
    "        Then, split the training instance features & labels according to the best splitting feature found,\n",
    "        and fit the Yes subtree with the instances that split to the True side,\n",
    "        and the No subtree with the instances that are False according to the splitting feature.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Your code here')\n",
    "\n",
    "        split, split_impurity = self._best_split(features, labels)  # Find the best split, if any\n",
    "\n",
    "        raise NotImplementedError('... and the rest of your code here')\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\" Predict the labels of the instances based on the features\n",
    "\n",
    "        :param features: pd.DataFrame of test features\n",
    "        :return: predicted labels\n",
    "\n",
    "        We start by initializing an array of labels where we naively predict this node's label.\n",
    "        The datatype of this array is set to `object` because otherwise numpy\n",
    "        might select the minimum needed string length for the current label, regardless of child labels.\n",
    "\n",
    "        Then if this is not a leaf node, we overwrite those values with the values of Yes and No child nodes,\n",
    "        based on the feature split in this node.\n",
    "        \"\"\"\n",
    "        results = np.full(features.shape[0], self._label, dtype=object)  # object!!!\n",
    "        if self._split:  # branch node; recursively replace predictions with child predictions\n",
    "            yes_index = features[self._split] > 0.5\n",
    "            results[yes_index] = self._yes.predict(features.loc[yes_index])\n",
    "            results[~yes_index] = self._no.predict(features.loc[~yes_index])\n",
    "        return results\n",
    "\n",
    "    def to_text(self, depth=0):\n",
    "        if self._split:\n",
    "            text = f'{\"|   \" * depth}|---{self._split} = no\\n'\n",
    "            text += self._no.to_text(depth=depth+1)\n",
    "            text += f'{\"|   \" * depth}|---{self._split} = yes\\n'\n",
    "            text += self._yes.to_text(depth=depth+1)\n",
    "            \n",
    "        else:\n",
    "            text = f'{\"|   \" * depth}|---{self._label} ({self._samples})\\n'.upper()\n",
    "        return text\n",
    "\n",
    "    def to_graphviz(self, choice='', parent='R', graph=None, size='15,15'):\n",
    "        details = f'\\n\\nimpurity = {self._impurity:.2f}\\nsamples = {self._samples}\\n{self._distribution}'\n",
    "        if self._split:\n",
    "            label = f'({self._label.lower()})'\n",
    "        else:\n",
    "            label = self._label.upper()\n",
    "        if graph is None:  # root node\n",
    "            graph = graphviz.Digraph()  # initialize the graph\n",
    "            graph.attr(size=size)\n",
    "            graph.attr(ratio='1.0')\n",
    "        if self._split:  # branching nodes\n",
    "            node_label = f'{label}\\n{self._split.upper()}???{details}'  # display name\n",
    "            graph.node(name=parent+choice, label=node_label, shape='diamond')\n",
    "            self._yes.to_graphviz(choice='yes', parent=parent+choice, graph=graph)\n",
    "            self._no.to_graphviz(choice='no', parent=parent+choice, graph=graph)\n",
    "        else:  # leaf node\n",
    "            node_label = f'{label}{details}'  # display name\n",
    "            graph.node(name=parent+choice, label=node_label, shape='rectangle')\n",
    "        if choice is not '':\n",
    "            graph.edge(parent, parent + choice, label=choice)  # draw arrow from parent to this one\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdYQy_gS8emK"
   },
   "source": [
    "A few notes about the class model shown above:\n",
    "* The appproach in this code is similar to algorithms 5.1 and 5.2 in the book by Peter Flach. Algorith 5.1 (GrowTree) corresponds loosely to `fit`, and algorithm 5.2 (BestSplit) corresponds to `_best_split`.\n",
    "* We implemented several of the components for you. This will be a \"fill in the blanks\" exercise. We already did `__init__`, `to_text` and `to_graphviz`. The init method sets the stage for the variables you need to use to build the tree. The other methods help you visualize what you're doing - you can use them to see if you're on the right track.\n",
    "* We will make use of recursion, because we don't know ahead of time how big a tree will need. After all, we want to be able to use our decision tree algorithm on any dataset!\n",
    "\n",
    "    We will do this by making the class just one node of a decision tree, but with the potential to have a Yes and No subtree. That way we use the definition of the Tree kind of datastructure, which says that the subtree of a tree is also a tree. So if the root DTree node splits, then the Yes and No side of it will also be DTree nodes, which might then get children of their own. All the way down, until we only have leaf nodes that aren't profitable to split anymore.\n",
    "\n",
    "* Implementing a class all at once can be hard. We've split up the task by making unit tests to test the steps one by one. You can use them to gradually develop your code:\n",
    "\n",
    "    1. `test_DTree_best_split` to develop the `_best_split` method that you'll need for your `fit` method.\n",
    "    2. `test_DTree_fit_basics` Before you even begin to decide whether to split the node, you have to record some information in the node.\n",
    "    3. `test_DTree_fit_children` If the node is split, then the Yes and No sides both should have a new node attached to it.\n",
    "    4. `test_DTree_fit_recursively_children` This takes the idea of the previous test, but now applies it recursively. If a node is split then its children should be nodes. And if those nodes are split, then *their* children should also be nodes. All the way until you get to nodes that aren't split.\n",
    "    5. `test_DTree_fit_recursively_child_labels` With the previous test we established that our tree consists of branching nodes that go Yes and No, or leaf nodes that don't split. Now we can go down the tree to check if all the nodes have a label.\n",
    "    6. `test_DTree_fit_recursively_decreasing_impurity` We should only be splitting if that results in a lower weighed impurity of the child nodes than that of the parent nodes. With this test we evaluate that recursively.\n",
    "    7. `test_DTree_fit_text_string` We output the model as a text string using the `to_text` method and check if it's the same as what we expect.\n",
    "\n",
    "* You may want to re-read the section in Lab1 about how to select rows in a Pandas DataFrame according to a condition (like the value in a feature column).\n",
    "\n",
    "* If you got stuck on designing a Gini and Entropy impurity measures in the exercises above, just focus on the parts of this exercise that use the minority_class impurity measure.\n",
    "\n",
    "* If you're a bit unsure about how to use `self` when desigining Python classes, there are many [articles](https://www.programiz.com/article/python-self-why) explaining why you see this word everywhere. Very, very broadly: it's just something that you include as an argument when designing a function, but you don't actually pass anything to it (that gets done for you automatically). You directly use it when you want to use something from *that instance* of the class. In this exercise for example, you're making multiple `DTree` objects. When you use a method from one of them and it uses `self.variable` to grab some variable, it's grabbing the variable from the same object as the method you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv9PmXAmozP5"
   },
   "source": [
    "#### Exercise 3: implement `_best_split`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQ93FxzKpf4-"
   },
   "source": [
    "First you should implement the `_best_split` method. The purpose of this method is to walk through all the available features that you *could* split on, and figure out for each of them how much the weighted impurity would be. Then, you return the name and weighted impurity of the best one. Since impurity is bad, that means the lowest weighted impurity.\n",
    "\n",
    "Weighted impurity means you compute the impurity for the Yes branch and weigh it by how many of the samples go into the Yes branch, and compute the impurity on the No branch and weigh it by how many of the samples go into the No branch, and then add them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyiOtt5D67nB"
   },
   "source": [
    "Use this dataset for the unit tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JppujMcs6_Pj"
   },
   "outputs": [],
   "source": [
    "# A toy dataset about judging the ripeness of avocados\n",
    "avocados = pd.DataFrame(data={\n",
    "        'green':     [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        'brown':     [1, 1, 0, 1, 1, 1, 0, 1],\n",
    "        'firmness':  [1, 1, 0, 1, 1, 0, 1, 1],\n",
    "        'softness':  [0, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'nub_loose': [0, 1, 1, 1, 0, 1, 1, 0],\n",
    "        'ripe':      [0, 1, 0, 1, 0, 0, 1, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcaifRHC7Lrb"
   },
   "source": [
    "And then work on it until it passes these unit tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijMwdl1C6XzO"
   },
   "outputs": [],
   "source": [
    "def test_DTree_best_split_minority():\n",
    "    # Using minority class\n",
    "    model = DTree(metric=minority_class)\n",
    "    X = avocados.iloc[:, :-1]\n",
    "    y = avocados.iloc[:, -1]\n",
    "    feature, impurity = model._best_split(X, y)\n",
    "    assert feature == 'firmness'\n",
    "    assert impurity == 0.25\n",
    "    \n",
    "\n",
    "test_DTree_best_split_minority()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWgS6wZf7Vy4"
   },
   "outputs": [],
   "source": [
    "def test_DTree_best_split_gini():\n",
    "    # Using gini\n",
    "    model = DTree(metric=gini)\n",
    "    X = avocados.iloc[:, :-1]\n",
    "    y = avocados.iloc[:, -1]\n",
    "    feature, impurity = model._best_split(X, y)\n",
    "    assert feature == 'firmness'\n",
    "    assert 0.33 < impurity < 0.34\n",
    "    \n",
    "\n",
    "test_DTree_best_split_gini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJXFXuti7V7o"
   },
   "outputs": [],
   "source": [
    "def test_DTree_best_split_entropy():\n",
    "    # Using entropy\n",
    "    model = DTree(metric=entropy)\n",
    "    X = avocados.iloc[:, [0, 1, 3, 4]]  # leave out firmness as feature\n",
    "    y = avocados.iloc[:, -1]\n",
    "    feature, impurity = model._best_split(X, y)\n",
    "    assert feature == 'softness'\n",
    "    assert 0.86 < impurity < 0.87\n",
    "\n",
    "    \n",
    "test_DTree_best_split_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd16UWlw7kAW"
   },
   "source": [
    "If you look carefully, you notice that when we initialize the model in these tests, we write `model = DTree(metric=minority_class)`, and similarily for `model = DTree(metric=gini)` and also for entropy.\n",
    "\n",
    "Although `minority_class(labels)` is a function, we don't write the parentheses and argument. This is because we're passing in the function *itself*, not the outcome of using it.\n",
    "\n",
    "This is easy in Python because functions are objects, just like integers, floats, lists and so forth. It allows us to easily configure how our code will work at runtime. If we come up with a new impurity measure, we don't have to change our DTree class, we just pass in the new function as an argument when initializing a new model. In some programming languages this technique is called *dependency injection*, and considered a very sophisticated design pattern. It's easy in Python though, but pretty useful. \n",
    "\n",
    "But you have to be careful because if you accidentally add then (), your code will not work correctly. Many tab-completion editors will tend to add the () when passing a function as argument like this, so keep an eye out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p312J3lxo2vq"
   },
   "source": [
    "#### Exercise 4: implement `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvXRWdAO_OT7"
   },
   "source": [
    "Now implement the `fit` method. The fitting process has two parts to it: fitting the current node, and maybe splitting it to have two child nodes. \n",
    "\n",
    "The first unit test is only considered with the first part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxXw5qne-r90"
   },
   "outputs": [],
   "source": [
    "def test_DTree_fit_basics():\n",
    "        \"\"\"\" Check if the fit function filled in these values \"\"\"\n",
    "        X = avocados.iloc[:, :-1]\n",
    "        y = avocados.iloc[:, -1]\n",
    "        model = DTree(metric=minority_class)\n",
    "        assert model._label is None, \"Before fitting, this should not be set yet.\"\n",
    "        assert model._impurity is None, \"Before fitting, this should not be set yet.\"\n",
    "        assert model._samples is None, \"Before fitting, this should not be set yet.\"\n",
    "        assert len(model._distribution) == 0, \"Before fitting, this should not be set yet.\"\n",
    "        model.fit(X, y)\n",
    "        assert model._label is not None, \"After fitting, we should know the majority label in the top node\"\n",
    "        assert model._impurity is not None, \"After fitting, we should know the impurity in the top node\"\n",
    "        assert isinstance(model._samples, int), \"After fitting, this count how many training samples reached this node\"\n",
    "        assert len(model._distribution) > 0, \"After fitting, this should store the frequency of each class in the node\"\n",
    "    \n",
    "    \n",
    "    test_DTree_fit_basics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7kiTkeB_zce"
   },
   "source": [
    "Using the avocado mini-dataset, the root node should be split because your `_best_split` method should have found a feature that has a lower weighed impurity than leaving things just as they are as a single node. So this test checks if the root node is split and has DTree child nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fX8hLhjJ_tv9"
   },
   "outputs": [],
   "source": [
    "def test_DTree_fit_children():\n",
    "    \"\"\" Check if the root node has split (it should) and has child nodes \"\"\"\n",
    "    X = avocados.iloc[:, :-1]\n",
    "    y = avocados.iloc[:, -1]\n",
    "    model = DTree(metric=minority_class)\n",
    "    assert model._split is False, \"Before fitting, this should not be set yet.\"\n",
    "    assert model._yes is None, \"Before fitting, this should not be set yet.\"\n",
    "    assert model._no is None, \"Before fitting, this should not be set yet.\"\n",
    "    model.fit(X, y)\n",
    "    assert model._split is not False, \"After fitting, the top node should have split\"\n",
    "    assert isinstance(model._yes, DTree), \"The Yes child node should be a subtree\"\n",
    "    assert isinstance(model._no, DTree), \"The No child node should be a subtree\"\n",
    "    \n",
    "    \n",
    "test_DTree_fit_children()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpMvWfQaAJxa"
   },
   "source": [
    "Now the idea is that you make these child nodes, and then also fit them, and they might get child nodes in turn. Any time a node is split, the `_yes` and `_no` side of it should be DTrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plbj-ID5_sL6"
   },
   "outputs": [],
   "source": [
    "def test_DTree_fit_recusively_children():\n",
    "    \"\"\" Check recursively if each node is either a leaf, or split and has two children \"\"\"\n",
    "    def recursive(model):\n",
    "        if model._split:\n",
    "            assert isinstance(model._yes, DTree), \"The Yes child node should be a subtree\"\n",
    "            assert isinstance(model._no, DTree), \"The No child node should be a subtree\"\n",
    "            recursive(model._yes)\n",
    "            recursive(model._no)\n",
    "    X = avocados.iloc[:, :-1]\n",
    "    y = avocados.iloc[:, -1]\n",
    "    model = DTree(metric=minority_class)\n",
    "    model.fit(X, y)\n",
    "    recursive(model)\n",
    "    \n",
    "    \n",
    "test_DTree_fit_recusively_children()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlXlred9Aa92"
   },
   "source": [
    "Each node should be labeled, not only the leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOlBeZHo_o2G"
   },
   "outputs": [],
   "source": [
    "def test_DTree_fit_recusively_child_labels():\n",
    "    \"\"\" Check recursively if each node is labeled \"\"\"\n",
    "    def recursive(model):\n",
    "        assert model._label is not None, \"Each node should be labeled.\"\n",
    "        if model._split:\n",
    "            recursive(model._yes)\n",
    "            recursive(model._no)\n",
    "    X = avocados.iloc[:, :-1]\n",
    "    y = avocados.iloc[:, -1]\n",
    "    model = DTree(metric=minority_class)\n",
    "    model.fit(X, y)\n",
    "    recursive(model)\n",
    "    \n",
    "    \n",
    "test_DTree_fit_recusively_child_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwVRPYkwAtmT"
   },
   "source": [
    "Splits should only happen if the weighed impurity of the children is lower than that of the parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi3QrIy3_mZq"
   },
   "outputs": [],
   "source": [
    "def test_DTree_fit_recusively_decreasing_impurity():\n",
    "    \"\"\" Check if the weighted impurity of children is always lower than that of the parent \"\"\"\n",
    "    def recursive_impurity(model):\n",
    "        if model._split:\n",
    "            yes_impurity, yes_samples = recursive_impurity(model._yes)\n",
    "            no_impurity, no_samples = recursive_impurity(model._no)\n",
    "            weighted_impurity = (yes_impurity * yes_samples) + (no_impurity * no_samples)\n",
    "            assert weighted_impurity < (model._impurity * model._samples), (\n",
    "                \"The weighted impurity of the children should be smaller than the parent\")\n",
    "        return model._impurity, model._samples\n",
    "    X = avocados.iloc[:, :-1]\n",
    "    y = avocados.iloc[:, -1]\n",
    "    model = DTree(metric=minority_class)\n",
    "    model.fit(X, y)\n",
    "    recursive_impurity(model)\n",
    "    \n",
    "    \n",
    "test_DTree_fit_recusively_decreasing_impurity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8BDJi0iA0EQ"
   },
   "source": [
    "Now we output the model we learned of these avocados as a text string, and compare it against what it *should* be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWOdOOzU_kq9"
   },
   "outputs": [],
   "source": [
    "def test_DTree_fit_text_string():\n",
    "    \"\"\" Check if we've learned the RIGHT model \"\"\"\n",
    "    X = avocados.iloc[:, :-1]\n",
    "    y = avocados.iloc[:, -1]\n",
    "    model = DTree(metric=minority_class)\n",
    "    model.fit(X, y)\n",
    "    text = model.to_text()\n",
    "    # Compare the text that you actually got to what it should be:\n",
    "    assert '\\n'+text == (\"\"\"\n",
    "|---firmness = no\n",
    "|   |---0 (2)\n",
    "|---firmness = yes\n",
    "|   |---nub_loose = no\n",
    "|   |   |---0 (3)\n",
    "|   |---nub_loose = yes\n",
    "|   |   |---1 (3)\n",
    "\"\"\"), \"The tree should look like this\"\n",
    "    \n",
    "    \n",
    "test_DTree_fit_text_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ0fn_g5eUwN"
   },
   "source": [
    "### Let's put our new decision tree to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmwD-m_UePjW"
   },
   "outputs": [],
   "source": [
    "# Dataset adapted from https://archive.ics.uci.edu/ml/datasets/Zoo\n",
    "url = 'https://raw.githubusercontent.com/MLCourse-LU/Datasets/main/zoo.csv'\n",
    "df = pd.read_csv(filepath_or_buffer=url, header=0, index_col=0)\n",
    "\n",
    "X = df.iloc[:, 1:-1]  # column 0 has the animal names, the last column has the labels\n",
    "y = df.iloc[:, -1]  # use the last column as labels\n",
    "\n",
    "\n",
    "model = DTree(metric=minority_class)\n",
    "# model = DTree(metric=gini)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrnWHRKwg0Dt",
    "outputId": "b70265ca-f7af-415b-eba8-1be81824b79e"
   },
   "outputs": [],
   "source": [
    "# Show it in text\n",
    "print(model.to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "zPd-codPee5X",
    "outputId": "f48b03bc-3b16-43cc-8fc5-edb17acdcba0"
   },
   "outputs": [],
   "source": [
    "viz = model.to_graphviz(size='10,10')\n",
    "viz.render('homebrew', format='pdf', cleanup=True)\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ComJxNvW_4w6"
   },
   "source": [
    "## Trees are sensitive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7hy4BMEhnZw"
   },
   "source": [
    "Let's make sure our data is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTvFwbcUAqHP"
   },
   "outputs": [],
   "source": [
    "# Dataset adapted from https://archive.ics.uci.edu/ml/datasets/Zoo\n",
    "url = 'https://raw.githubusercontent.com/MLCourse-LU/Datasets/main/zoo.csv'\n",
    "df = pd.read_csv(filepath_or_buffer=url, header=0, index_col=0)\n",
    "\n",
    "X = df.iloc[:, 1:-1]  # column 0 has the animal names, the last column has the labels\n",
    "y = df.iloc[:, -1]  # use the last column as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEUc3ix9AMLY"
   },
   "source": [
    "Now, suppose you wanted to know whether the Gini or the Minority Class impurity measure was better for fitting the tree. And you're using accuracy to score your model performance with. You can run the code below to fit a model, then make some predictions with it and evaluate the accuracy.\n",
    "\n",
    "**Try it multiple times, both for the Gini and the Minority Class model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "SZdnRQPvA5tU",
    "outputId": "b98da88f-8484-43f0-bcd9-58e2304630ae"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "model = DTree(metric=minority_class)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "viz = model.to_graphviz(size='10,10')\n",
    "viz.render('homebrew_gini', format='pdf', cleanup=True)\n",
    "viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "guE1PROrBXzY",
    "outputId": "3226cf18-8444-4497-8609-a3ec6dec9324"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "model = DTree(metric=minority_class)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "viz = model.to_graphviz(size='10,10')\n",
    "viz.render('homebrew_minority', format='pdf', cleanup=True)\n",
    "viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dmmsEUiAl65"
   },
   "source": [
    "### Open Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you had trouble implementing the tree, you may also use a scikit-learn decision tree to figure out the answers to these questions. The results should be very similar.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQVKCoLuh3sV"
   },
   "source": [
    "#### Question 3: Why are the trees not always the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMwPI46wh_GX"
   },
   "source": [
    "#### Question 4: Choose a tree with particularly bad accuracy score. Put the graph as a picture in your report. Explain what you think may have happened during the fitting to make this such a bad model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Xk7s-xvC3bt"
   },
   "source": [
    "## Configuration using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WPe70dhaC5H"
   },
   "source": [
    "In machine learning we often want to know \"what would be a good model type for this dataset?\", or \"given this model we want to use, how should we configure it?\" But the examples above here showed that building just *one* tree on *one* way to split the data was not a reliable method to determine which choice of model/parameters was the best.\n",
    "\n",
    "The obvious thing to try of course is just building more than one and taking the average. But if you just use the same data every time, you'll get the same tree too. Some other ML algorithms might also give you a different model even if you use the same data. In our implementation of the decision tree however, once you've decided which data you're going to train it on, the result is deterministic.\n",
    "\n",
    "So what's really needed is doing more with our data. Specifically, we want to make sure every instance in our dataset is used at least once to test with. But we don't want to train our model on data that we're testing with. So we need some kind of rotation. And that's what cross-validation is, basically.\n",
    "\n",
    "We often use $k$-fold cross validation, where $k$ is the number of \"folds\" we have. Each fold is a different way of splitting our dataset, and the *test set* in each fold is different from the test sets of all the other folds. So if we had 5-fold cross validation, each fold would use 20% of the data as a test set, and the rest as a training set. \n",
    "\n",
    "We then build our model $k$ times, so 5 different decision trees, and score each one on its test set. Then we aggregate those scores, for example by taking the average and also observing the standard deviation. These numbers are a better indication of how good our model/configuration choice is than a single run.\n",
    "\n",
    "Consider the implementation of $k$-fold cross validation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGJMl2_YR3zE"
   },
   "outputs": [],
   "source": [
    "class KFolds:\n",
    "    def __init__(self, X, y, k, seed=None):\n",
    "        \"\"\" Initialize the KFolds instance\n",
    "        \n",
    "        :param X: pd.DataFrame of feature columns \n",
    "        :param y: pd.DataFrame or pd.Series of labels\n",
    "        :param k: number of folds desired\n",
    "        :param seed: random seed, if you want reproducible results (optional)\n",
    "        \n",
    "        After initialization, self.folds will store k folds.\n",
    "        Each fold is a pair of arrays with training indices and test indices.\n",
    "        The folds are as evenly distributed in size as possible.\n",
    "        All the test segments are pairwise disjoint.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.k = k\n",
    "        self.folds = []\n",
    "        indices = np.arange(X.shape[0])  # integer indices of the instances\n",
    "        if seed is not None:  # Set random seed if desired.\n",
    "            np.random.seed(seed=seed)\n",
    "        np.random.shuffle(indices)  # Shuffle in-place.\n",
    "        fold_size = X.shape[0] / k  # How many instances per fold? Note that this is a floating point number!\n",
    "        for fold_num in range(k):\n",
    "            # The int() is used to handle the floating point numbers and make the segments as equal as possible.\n",
    "            test = indices[int(fold_num * fold_size):int((fold_num + 1) * fold_size)]\n",
    "            train = np.concatenate([indices[:int(fold_num * fold_size)], indices[int((fold_num + 1) * fold_size):]])\n",
    "            self.folds.append((train, test))\n",
    "\n",
    "    def get_fold(self, fold_num):\n",
    "        \"\"\" Get the training and test data of the k-th fold\n",
    "        \n",
    "        :param fold_num: Which fold's division of the data to use\n",
    "        :return: Training and test features/labels\n",
    "        \"\"\"\n",
    "        train, test = self.folds[fold_num]  # Select the indices developed for this fold during initialization\n",
    "        # Use those indices to select instance rows to send to test and training sets.\n",
    "        X_train = self.X.iloc[train]\n",
    "        X_test = self.X.iloc[test]\n",
    "        y_train = self.y.iloc[train]\n",
    "        y_test = self.y.iloc[test]\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjKHCv57XOcJ"
   },
   "source": [
    "That's a big wall of code, how do you use it? A lot like our earlier `train_test_split` function, but now you request a specific fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "BCYA6N1mXRfB",
    "outputId": "9a32b094-3ed7-4c26-9113-53680f1cf1f1"
   },
   "outputs": [],
   "source": [
    "folds = KFolds(X, y, k=5)  # set it up by putting data in\n",
    "X_train, X_test, y_train, y_test = folds.get_fold(fold_num=3)  # Now get the data out, divided according to fold 3\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r33XENttYBPm"
   },
   "source": [
    "You can use those folds of the dataset with your own decision tree, or with the Scikit-Learn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4by7aqjjgiC"
   },
   "source": [
    "### Open Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2Fc9Jpwdyc5"
   },
   "source": [
    "#### Question 5: Make a table comparing the different model/configurations.\n",
    "Write a loop that for a given value of $k$ applies k-fold cross validation with the zoo dataset. For $k=5$, report in a table the **mean** and **standard deviation** of the `accuracy_score` of each of the following:\n",
    "* The Scikit-Learn tree, using Gini\n",
    "* The Scikit-Learn tree, using Entropy\n",
    "* Your own tree, using Gini\n",
    "* Your own tree, using Entropy\n",
    "* Your own tree, using Minority Class\n",
    "\n",
    "(For full credit, do at least the Scikit-Learn trees or your own. Doing both counts as a bonus question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heshcakXYKAd"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAMBiWOZiVBn"
   },
   "source": [
    "#### Question 6: which would you prefer? \n",
    "A high-accuracy model with large standard deviation, or a somewhat lower accuracy model with lower standard deviation? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8oc7dcuiiW3"
   },
   "source": [
    "#### Question 7: Experiment with different values of $k$. \n",
    "What is the effect on the accuracy and standard deviation of using larger or smaller $k$? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw-bnKOKYAJk"
   },
   "source": [
    "### Comparing ours to theirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czGdIzaZY1jh"
   },
   "source": [
    "You can also compare whether your models are working like they should by providing both your own `DTree` implementation and the Scikit-Learn `DecisionTreeClassifier` the same data and then using their text export functions to see what model they've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "En8nUJZCVJU7",
    "outputId": "f05fcaca-5bec-48a3-c4a5-eca1d80e6caf"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'SK {accuracy_score(y_test, y_pred)}')\n",
    "print(export_text(model, feature_names=fn))\n",
    "\n",
    "\n",
    "model = DTree(gini)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Ours {accuracy_score(y_test, y_pred)}')\n",
    "print(model.to_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3cFbO_RhFGG",
    "outputId": "0d1ed836-35d8-41ff-8169-b7a7dd1ec590"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'SK {accuracy_score(y_test, y_pred)}')\n",
    "print(export_text(model, feature_names=fn))\n",
    "\n",
    "\n",
    "model = DTree(entropy)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Ours {accuracy_score(y_test, y_pred)}')\n",
    "print(model.to_text())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A2_teacher.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
